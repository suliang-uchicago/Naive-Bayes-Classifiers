{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b31ac553",
   "metadata": {},
   "source": [
    "### Naive Bayes classifiers\n",
    "- Gaussian Naive Bayes\n",
    "- Multinomial Naive Bayes\n",
    "- Bernoulli Naive Bayes\n",
    "- Categorical Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "710f0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f569e315",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes\n",
    "- continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b10659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2621f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                        test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce450133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 50, 1: 50, 2: 50})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb4ed412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b11bb08",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.6, 3.1, 1.5, 0.2],\n",
       "       [5.9, 3. , 5.1, 1.8],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [4.6, 3.2, 1.4, 0.2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48051880",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681a183f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 75 points : 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f58eef",
   "metadata": {},
   "source": [
    "- Explore model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc4e110d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29., 20., 26.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training data class distribution \n",
    "gnb.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5269b1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.387, 0.267, 0.347])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(gnb.class_prior_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5e60790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc67fa06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.103, 0.132, 0.016, 0.008],\n",
       "       [0.256, 0.083, 0.255, 0.046],\n",
       "       [0.389, 0.101, 0.313, 0.048]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variance of each feature per class\n",
    "np.round(gnb.var_, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0248e3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.976, 3.359, 1.448, 0.234],\n",
       "       [5.935, 2.71 , 4.185, 1.3  ],\n",
       "       [6.777, 3.092, 5.735, 2.108]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean of each feature per class\n",
    "np.round(gnb.theta_, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027436b",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes\n",
    "- primarily used in text classification\n",
    "- data are typically represented as count vector representation of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "874fcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "227d9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news_train = fetch_20newsgroups(subset='train', \n",
    "                                categories=['alt.atheism','comp.graphics'], \n",
    "                                shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd4e34f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e78a1ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 480, 1: 584})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(news_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ec8ed0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1064, 4230)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the training data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words='english', max_df=0.8, min_df=5)\n",
    "X_train_vec = count_vect.fit_transform(news_train.data)\n",
    "X_train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb5a5550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the classifier\n",
    "# self-explore other parameter settings: alpha, class_prior\n",
    "mnb = MultinomialNB(alpha=1.0, class_prior=None)  \n",
    "mnb.fit(X_train_vec, news_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d5f2c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(708, 4230)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the test data\n",
    "news_test = fetch_20newsgroups(subset='test', \n",
    "                               categories=['alt.atheism','comp.graphics'], \n",
    "                               shuffle=True, random_state=42)\n",
    "\n",
    "X_test_vec = count_vect.transform(news_test.data)\n",
    "X_test_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e544901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the fitted classifier to predict for test data\n",
    "y_pred = mnb.predict(X_test_vec)\n",
    "\n",
    "# model accuracy\n",
    "np.round(mnb.score(X_test_bi, news_test.target), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd5a6161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled data out of a total 708 data : 11\n"
     ]
    }
   ],
   "source": [
    "# check wrong predictions \n",
    "print(\"Number of mislabeled data out of a total %d data : %d\"\n",
    "      % (X_test_vec.shape[0], (news_test.target != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b58fd",
   "metadata": {},
   "source": [
    "- Explore model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "456b642e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([480., 584.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training data class distribution\n",
    "mnb.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "235664cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.796, -0.6  ])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smoothed empirical log probability for each class\n",
    "np.round(mnb.class_log_prior_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ef5cada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -9.793,  -7.624,  -8.614, ..., -11.179, -10.486, -11.179],\n",
       "       [ -7.53 ,  -8.195,  -8.141, ...,  -8.687,  -7.153,  -8.687]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empirical log probability of features given a class, P(x_i|y).\n",
    "np.round(mnb.feature_log_prob_,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f353b",
   "metadata": {},
   "source": [
    "#### Bernoulli Naive Bayes\n",
    "- Assumes each feature is a binary-valued variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "94c046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8961a254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1064, 4230)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same dataset but with binary vectorization\n",
    "bi_vect = CountVectorizer(stop_words='english', binary=True,\n",
    "                             max_df=0.8, min_df=5)\n",
    "X_train_bi = bi_vect.fit_transform(news_train.data)\n",
    "X_train_bi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81e9f736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(708, 4230)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the test data\n",
    "X_test_bi = bi_vect.transform(news_test.data)\n",
    "X_test_bi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "317e056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the classifier \n",
    "# self-explore other parameter settings\n",
    "bnb = BernoulliNB(alpha=1.0, fit_prior=True, class_prior=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7703ac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.952"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test and the classifier\n",
    "y_pred = bnb.fit(X_train_bi,news_train.target).predict(X_test_bi)\n",
    "# model accuracy\n",
    "np.round(bnb.score(X_test_bi, news_test.target), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a164c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled data out of a total 708 data : 34\n"
     ]
    }
   ],
   "source": [
    "# check wrong predictions \n",
    "print(\"Number of mislabeled data out of a total %d data : %d\"\n",
    "      % (X_test_bi.shape[0], (news_test.target != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc707e2",
   "metadata": {},
   "source": [
    "- Explore model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf878b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([480., 584.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training data class distribution\n",
    "bnb.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6d58ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.796, -0.6  ])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smoothed empirical log probability for each class\n",
    "np.round(bnb.class_log_prior_, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6dde9792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.792, -3.693, -3.613, ..., -6.178, -5.485, -6.178],\n",
       "       [-3.329, -3.888, -3.975, ..., -3.975, -3.429, -3.975]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empirical log probability of features given a class, P(x_i|y).\n",
    "np.round(bnb.feature_log_prob_, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3d37c",
   "metadata": {},
   "source": [
    "#### Categorical Naive Bayes\n",
    "- Assumes each feature has a categorical distribution \n",
    "- All features are encoded (OrdinalEncoder, OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3a462fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e39b9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = rng.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1820154d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 100)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "# 6 samples, each sample represented by 100 features\n",
    "# each feature is encoded as a categorical variable in between [0,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "93007571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 4, 0, 1, 3, 0, 0, 1, 4, 4, 1, 2, 4, 2, 4, 3, 4, 2, 4, 2, 4, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 4, 1, 0, 0, 3, 2, 1, 0, 3, 1, 1, 3, 4, 0, 1,\n",
       "        3, 4, 2, 4, 0, 3, 1, 2, 0, 4, 1, 2, 2, 1, 0, 1, 3, 4, 3, 1, 3, 0,\n",
       "        0, 2, 2, 1, 3, 4, 2, 0, 0, 1, 1, 3, 0, 0, 4, 2, 4, 3, 3, 0, 3, 4,\n",
       "        3, 4, 4, 4, 1, 0, 4, 2, 0, 2, 4, 1],\n",
       "       [1, 0, 2, 4, 4, 0, 4, 1, 4, 1, 0, 2, 3, 1, 2, 4, 4, 2, 2, 0, 1, 2,\n",
       "        2, 0, 1, 2, 4, 0, 1, 2, 1, 4, 2, 0, 0, 1, 0, 1, 3, 1, 1, 4, 4, 3,\n",
       "        0, 3, 0, 3, 1, 2, 4, 0, 0, 3, 1, 1, 0, 0, 4, 2, 3, 4, 2, 0, 3, 3,\n",
       "        1, 2, 4, 3, 0, 0, 4, 2, 4, 2, 0, 3, 0, 0, 4, 2, 1, 0, 4, 3, 0, 1,\n",
       "        2, 4, 4, 3, 3, 3, 3, 2, 3, 3, 4, 3],\n",
       "       [2, 4, 4, 0, 3, 3, 0, 3, 1, 0, 2, 2, 2, 0, 2, 1, 4, 0, 4, 4, 1, 3,\n",
       "        1, 4, 1, 2, 1, 0, 0, 2, 4, 1, 0, 0, 3, 1, 0, 4, 3, 2, 3, 4, 4, 3,\n",
       "        0, 0, 0, 4, 1, 4, 1, 2, 2, 4, 3, 4, 4, 0, 3, 2, 4, 3, 4, 2, 3, 0,\n",
       "        2, 1, 3, 2, 0, 1, 4, 1, 3, 3, 1, 2, 0, 2, 4, 0, 2, 4, 3, 4, 3, 0,\n",
       "        4, 2, 2, 4, 1, 2, 1, 1, 1, 0, 4, 4],\n",
       "       [2, 2, 3, 1, 4, 0, 0, 3, 2, 4, 1, 3, 1, 1, 2, 4, 0, 3, 0, 4, 2, 3,\n",
       "        1, 1, 4, 4, 0, 2, 1, 3, 0, 1, 0, 2, 2, 4, 3, 2, 2, 2, 0, 2, 0, 4,\n",
       "        1, 0, 2, 3, 0, 4, 3, 3, 3, 0, 3, 1, 2, 0, 1, 4, 2, 3, 4, 4, 2, 1,\n",
       "        2, 0, 3, 3, 2, 0, 0, 0, 0, 2, 4, 0, 4, 1, 2, 1, 2, 4, 1, 3, 1, 1,\n",
       "        2, 4, 1, 0, 2, 1, 2, 0, 0, 3, 4, 1],\n",
       "       [0, 4, 0, 3, 2, 4, 3, 2, 4, 2, 4, 0, 0, 4, 2, 2, 4, 2, 3, 0, 0, 4,\n",
       "        3, 4, 3, 3, 4, 0, 3, 1, 4, 4, 3, 2, 2, 2, 2, 2, 0, 2, 1, 2, 3, 0,\n",
       "        0, 1, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 0, 4, 0, 2, 4, 4, 2, 0, 3, 2,\n",
       "        4, 0, 4, 2, 3, 4, 2, 4, 1, 3, 4, 3, 0, 3, 0, 4, 3, 0, 3, 1, 4, 4,\n",
       "        2, 2, 4, 2, 1, 2, 3, 1, 3, 3, 0, 4],\n",
       "       [3, 3, 3, 3, 0, 2, 3, 1, 3, 2, 3, 2, 2, 0, 4, 0, 1, 3, 0, 0, 0, 1,\n",
       "        2, 4, 4, 2, 0, 1, 0, 0, 2, 4, 3, 0, 2, 1, 3, 3, 1, 4, 4, 4, 0, 1,\n",
       "        0, 1, 2, 2, 4, 2, 4, 0, 2, 4, 4, 1, 1, 1, 1, 4, 3, 4, 1, 1, 1, 2,\n",
       "        4, 3, 2, 3, 3, 2, 3, 0, 2, 0, 1, 0, 0, 3, 4, 0, 0, 0, 0, 1, 0, 1,\n",
       "        3, 3, 4, 0, 1, 0, 3, 2, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ffeef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the classifier \n",
    "# self-explore different parameter settings\n",
    "cnb = CategoricalNB(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4144055d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test and the classifier\n",
    "y_pred = cnb.fit(X,y)\n",
    "\n",
    "# model prediction for a specific sample\n",
    "cnb.predict(X[2:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723c3d5",
   "metadata": {},
   "source": [
    "- Explore model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ae72440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training data class distribution\n",
    "cnb.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c35e7513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (6 classes, 5 categories) for 100 features\n",
    "# cnb.category_count_ # 100*6*5\n",
    "\n",
    "cnb.category_count_[1] # category count for the first feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "658df571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.792, -1.792, -1.792, -1.792, -1.792, -1.792])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smoothed empirical log probability for each class\n",
    "np.round(cnb.class_log_prior_, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8213de19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.792, -1.792, -1.792, -1.792, -1.099],\n",
       "       [-1.099, -1.792, -1.792, -1.792, -1.792],\n",
       "       [-1.792, -1.792, -1.792, -1.792, -1.099],\n",
       "       [-1.792, -1.792, -1.099, -1.792, -1.792],\n",
       "       [-1.792, -1.792, -1.792, -1.792, -1.099],\n",
       "       [-1.792, -1.792, -1.792, -1.099, -1.792]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empirical log probability of categories given the respective feature and class, P(x_i|y).\n",
    "# corresponds to category_count_\n",
    "np.round(cnb.feature_log_prob_[1], 3) # log prob for the first feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7f64e1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5,\n",
       "       4, 5, 5, 5, 5, 3, 4, 5, 5, 5, 4, 4, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5,\n",
       "       4, 5, 3, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4,\n",
       "       5, 4, 5, 4, 4, 5, 5, 5, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 4, 4, 5, 3, 4, 4, 5, 5])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of categories each feature has\n",
    "cnb.n_categories_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
